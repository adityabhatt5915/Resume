{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10118657-d004-4026-b670-73074e6a6a53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Found 400 images belonging to 10 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x2000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "39/40 [============================>.] - ETA: 0s - loss: 3.0844 - accuracy: 0.6179WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "40/40 [==============================] - 2s 23ms/step - loss: 3.0128 - accuracy: 0.6275 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "39/40 [============================>.] - ETA: 0s - loss: 0.0610 - accuracy: 1.0000WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "40/40 [==============================] - 1s 20ms/step - loss: 0.0603 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 1.0000WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "40/40 [==============================] - 1s 24ms/step - loss: 0.0170 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "38/40 [===========================>..] - ETA: 0s - loss: 0.0097 - accuracy: 1.0000WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "40/40 [==============================] - 1s 24ms/step - loss: 0.0095 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 1.0000WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "40/40 [==============================] - 1s 21ms/step - loss: 0.0066 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "40/40 [==============================] - 1s 22ms/step - loss: 0.0050 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "39/40 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "40/40 [==============================] - 1s 21ms/step - loss: 0.0040 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "40/40 [==============================] - 1s 28ms/step - loss: 0.0033 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "39/40 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "40/40 [==============================] - 1s 24ms/step - loss: 0.0028 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "39/40 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "40/40 [==============================] - 1s 21ms/step - loss: 0.0024 - accuracy: 1.0000 - lr: 0.0010\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input data to be non-empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m imgs, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(train_batches)\n\u001b[0;32m     71\u001b[0m imgs, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(test_batches)\n\u001b[1;32m---> 72\u001b[0m scores \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(imgs, labels, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mmetrics_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mmetrics_names[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     76\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mASUS\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msign-language-recognition-project\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mbest_model_dataflair3.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1319\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_dataset_and_inferred_steps(\n\u001b[0;32m   1315\u001b[0m     strategy, x, steps_per_epoch, class_weight, distribute\n\u001b[0;32m   1316\u001b[0m )\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected input data to be non-empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input data to be non-empty."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "train_path = r'C:\\\\Users\\\\ASUS\\\\Downloads\\\\sign-language-recognition-project\\\\code\\\\gesture\\\\train'\n",
    "test_path = r'C:\\\\Users\\\\\\ASUS\\\\Downloads\\\\sign-language-recognition-project\\\\code\\\\gesture\\\\test'\n",
    "\n",
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), class_mode='categorical', batch_size=10,shuffle=True)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), class_mode='categorical', batch_size=10, shuffle=True)\n",
    "\n",
    "imgs, labels = next(train_batches)\n",
    "\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plotImages(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(10,activation =\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "history2 = model.fit(train_batches, epochs=10, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)\n",
    "imgs, labels = next(train_batches)\n",
    "\n",
    "imgs, labels = next(test_batches)\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "\n",
    "model.save(r'C:\\\\Users\\\\ASUS\\Downloads\\\\sign-language-recognition-project\\\\code\\\\best_model_dataflair3.h5')\n",
    "\n",
    "print(history2.history)\n",
    "\n",
    "imgs, labels = next(test_batches)\n",
    "\n",
    "model = keras.models.load_model(r'C:\\\\Users\\\\ASUS\\\\Downloads\\\\sign-language-recognition-project\\\\code\\\\best_model_dataflair3.h5')\n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "scores \n",
    "model.metrics_names\n",
    "\n",
    "word_dict = {0:'One',1:'Ten',2:'Two',3:'Three',4:'Four',5:'Five',6:'Six',7:'Seven',8:'Eight',9:'Nine'}\n",
    "\n",
    "predictions = model.predict(imgs, verbose=0)\n",
    "print(\"predictions on a small set of test data--\")\n",
    "print(\"\")\n",
    "for ind, i in enumerate(predictions):\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "plotImages(imgs)\n",
    "print('Actual labels')\n",
    "for i in labels:\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "print(imgs.shape)\n",
    "\n",
    "history2.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28265920-26af-4aa6-b77f-d94a190eec6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (file signature not found)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mASUS\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msign-language-recognition-project\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mbest_model_dataflair3.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m word_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOne\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m1\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTen\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m2\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTwo\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m3\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThree\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m4\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFour\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m5\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFive\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m6\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSix\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m7\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSeven\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m8\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEight\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m9\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNine\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m      9\u001b[0m background \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    255\u001b[0m         filepath,\n\u001b[0;32m    256\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    258\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    263\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    264\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\h5py\\_hl\\files.py:533\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, **kwds)\u001b[0m\n\u001b[0;32m    525\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    526\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    527\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    528\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    529\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    530\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    531\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    532\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 533\u001b[0m     fid \u001b[38;5;241m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[38;5;241m=\u001b[39mswmr)\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\h5py\\_hl\\files.py:226\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    225\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 226\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, flags, fapl\u001b[38;5;241m=\u001b[39mfapl)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    228\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (file signature not found)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "model = keras.models.load_model(r'C:\\\\Users\\\\ASUS\\\\Downloads\\\\sign-language-recognition-project\\\\code\\\\best_model_dataflair3.h5')\n",
    "word_dict = {0:'One',1:'Ten',2:'Two',3:'Three',4:'Four',5:'Five',6:'Six',7:'Seven',8:'Eight',9:'Nine'}\n",
    "\n",
    "background = None\n",
    "accumulated_weight = 0.5\n",
    "\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_right = 150\n",
    "ROI_left = 350\n",
    "\n",
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "\n",
    "    global background\n",
    "    \n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)\n",
    "\n",
    "def segment_hand(frame, threshold=25):\n",
    "    global background\n",
    "    \n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "    _ , thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)\n",
    "        return (thresholded, hand_segment_max_cont)\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "num_frames =0\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_copy = frame.copy()\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "\n",
    "    gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "\n",
    "    if num_frames < 70:\n",
    "        \n",
    "        cal_accum_avg(gray_frame, accumulated_weight)\n",
    "        cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\", (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "    \n",
    "    else: \n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        if hand is not None:\n",
    "            \n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right, ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.imshow(\"Thesholded Hand Image\", thresholded)\n",
    "            \n",
    "            thresholded = cv2.resize(thresholded, (64, 64))\n",
    "            thresholded = cv2.cvtColor(thresholded, cv2.COLOR_GRAY2RGB)\n",
    "            thresholded = np.reshape(thresholded, (1,thresholded.shape[0],thresholded.shape[1],3))\n",
    "            \n",
    "            pred = model.predict(thresholded)\n",
    "            cv2.putText(frame_copy, word_dict[np.argmax(pred)], (170, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "     \n",
    "    cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right, ROI_bottom), (255,128,0), 3)\n",
    "\n",
    "    num_frames += 1\n",
    "\n",
    "    cv2.putText(frame_copy, \"DataFlair hand sign recognition_ _ _\", (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "    cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f98cc9-e2e4-48a6-b415-33313a3dbd16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
